# ğŸ¤– Question Answering System Using BERT in Python

## Context-Aware QA Pipeline with BERT and Word2Vec

### MSc Final Project â€“ Data Science & Business Informatics â€“ University of Pisa

## ğŸ“Œ Overview
This project demonstrates the development of a Contextual Question Answering System (QAS) using BERT (Bidirectional Encoder Representations from Transformers) and Word2Vec in Python. The goal is to build a system that reads a given article and accurately answers user questions based on contextâ€”enhancing traditional information retrieval with deep NLP techniques.

By leveraging pretrained transformer models and semantic embeddings, this QAS goes beyond keyword matching and retrieves contextually correct answers, showing the power of modern language models in real-world applications.

## ğŸ¯ Objectives
 â—  Develop a QAS that understands natural language context and returns precise answers

 â—  Fine-tune BERT for SQuAD-style question answering

 â—  Implement a Word2Vec-based semantic similarity baseline

 â—  Compare both models for performance and accuracy

 â—  Handle long-text limitations of transformer models through smart chunking and preprocessing

## ğŸ§  Model Architecture
### 1. Word2Vec-Based QA (Baseline)
âœ¦ Trained on dataset with cleaned question-answer pairs

âœ¦ Questions and text passages embedded using Word2Vec

âœ¦ Answers selected based on cosine similarity between question and sentence embeddings

### 2. BERT-Based QA
âœ¦ Used HuggingFace BertForQuestionAnswering model

âœ¦ Fine-tuned on Q&A data following SQuAD format

âœ¦ Employed chunking (512 tokens + stride) to handle large article inputs

âœ¦ Selected the most probable answer span based on model confidence

## ğŸ› ï¸ Tools & Technologies
â¤ Languages: Python

â¤ Frameworks: PyTorch, Transformers (HuggingFace), Gensim

â¤ Libraries: transformers, torch, sklearn, pandas, matplotlib

â¤ IDE: Jupyter Notebook / Anaconda

## ğŸ§ª Dataset
Source: Public Question-Answer dataset curated by [Rachael Tatman]

â¤ Features:

â¤ Article Title

â¤ Question Answer Difficulty (from questioner and answerer) Associated Wikipedia article text Preprocessing involved merging yearly subsets, cleaning noisy tokens, and handling missing data

## âš™ï¸ Pipeline Architecture

graph TD;
    A[Load Dataset] --> B[Preprocessing & Cleaning];
    B --> C[Word2Vec Embedding];
    B --> D[BERT Tokenization];
    C --> E[Cosine Similarity Matching];
    D --> F[BERT Span Prediction];
    E --> G[Answer Selection];
    F --> G;
## ğŸ“ˆ Results

â¤ Model	Task	Performance Highlights
â¤ Word2Vec	Sentence Similarity QA	Reasonable accuracy using cosine similarity
â¤ BERT	Span-Based QA (SQuAD format)	High precision answers in long-context data
â¤ BERT Chunking	Handles long texts > 512 tokens	Achieved accurate span predictions
### ğŸ“Œ Output: BERT consistently returned more context-aware answers, while Word2Vec showed strong performance in basic semantic retrieval.

## ğŸ” Key Features
âœ¦ Handles long contexts using BERT stride-based chunking

âœ¦ Interactive answer prediction for any given article and question

âœ¦ Visualizations of model performance and predictions

âœ¦ Scalable architecture for extension into chatbots or virtual assistants

## ğŸ‘¨â€ğŸ“ Author
### Amir Hassan
### ğŸ“§ amirhassanunipi29@gmail.com
### ğŸ“ Masterâ€™s in Data Science & Business Informatics â€“ UniversitÃ  di Pisa, Italy
